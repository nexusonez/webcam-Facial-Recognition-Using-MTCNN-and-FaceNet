{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayTBV50uWeFA"
      },
      "source": [
        "1. Install python then setup MTCNN and FaceNet. Deep-learning frameworks like tensorflow and pytorch are required to run MTCNN and FaceNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4PRhARKVhR9",
        "outputId": "91dad288-d05d-46b2-ee4f-8a409cb5cc9f"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip install opencv-python\n",
        "# https://github.com/timesler/facenet-pytorch\n",
        "# In this repository, it also includs MTCNN for face detection prior to inference.\n",
        "# Making this the fastest MTCNN implementation available.\n",
        "%pip install facenet-pytorch\n",
        "%pip install MTCNN\n",
        "%pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install opencv-contrib-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU9d3Z6aXD4h"
      },
      "source": [
        "2. Initialize MTCNN and FaceNet Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoyxwiLSgQ3m"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR3SEQ9OdBVq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# If required, create a face detection pipeline using MTCNN:\n",
        "mtcnn = MTCNN(image_size=576, margin=0)\n",
        "\n",
        "# Create an inception resnet (in eval mode):\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aYb68iAap9t"
      },
      "source": [
        "3. Process an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk_KdtPZasA8"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "img = Image.open(r\"C:\\Users\\kamka\\OneDrive\\Desktop\\CMP6200 Project\\Facial Recognition using MTCNN and FaceNet\\sample_data\\Karhou2.jpg\")\n",
        "\n",
        "# Convert the image to RGB\n",
        "img_rgb = img.convert(\"RGB\")\n",
        "\n",
        "# Get cropped and prewhitened image tensor\n",
        "img_cropped = mtcnn(img_rgb, save_path=r\"C:\\Users\\kamka\\OneDrive\\Desktop\\CMP6200 Project\\Facial Recognition using MTCNN and FaceNet\\cropped_imgdata\\Karhou2.jpg\")\n",
        "\n",
        "# Calculate embedding (unsqueeze to add batch dimension)\n",
        "img_embedding = resnet(img_cropped.unsqueeze(0))\n",
        "\n",
        "# Or, if using for VGGFace2 classification\n",
        "resnet.classify = True\n",
        "img_probs = resnet(img_cropped.unsqueeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Try with camera and make a bounding box around a face frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install opencv-python opencv-python-headless mtcnn\n",
        "%pip install mtcnn\n",
        "%pip install tensorflow\n",
        "%pip install matplotlib\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Open Camera to video capture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "stream = cv2.VideoCapture(0)\n",
        "\n",
        "if not stream.isOpened():\n",
        "    print(\"No stream available\")\n",
        "    exit()\n",
        "    \n",
        "while (True):\n",
        "    ret, frame = stream.read()\n",
        "    if not ret:\n",
        "        print(\"No stream available\")\n",
        "        break\n",
        "        \n",
        "    cv2.imshow(\"Webcam\", frame)\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "        \n",
        "stream.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +\n",
        "                    \"haarcascade_frontalface_default.xml\")\n",
        "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +\n",
        "                    \"haarcascade_smile.xml\")\n",
        "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +\n",
        "                    \"haarcascade_eye.xml\")\n",
        "\n",
        "def detect_features(frame):\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "            # You can also specify minSize and maxSize\n",
        "    for (x, y, w, h) in faces:\n",
        "        frame = cv2.rectangle(frame, (x, y), (x+w, y+h),\n",
        "                            color=(0, 255, 0), thickness=5)\n",
        "        face = frame[y : y+h, x : x+w]\n",
        "        gray_face = gray[y : y+h, x : x+w]\n",
        "        smiles = smile_cascade.detectMultiScale(gray_face, \n",
        "                            2.5, minNeighbors=9)\n",
        "        for (xp, yp, wp, hp) in smiles:\n",
        "            face = cv2.rectangle(face, (xp, yp), (xp+wp, yp+hp),\n",
        "                    color=(0, 0, 255), thickness=5)\n",
        "        \n",
        "        eyes = eye_cascade.detectMultiScale(gray_face, \n",
        "                    2.5, minNeighbors=7)\n",
        "        for (xp, yp, wp, hp) in eyes:\n",
        "            face = cv2.rectangle(face, (xp, yp), (xp+wp, yp+hp),\n",
        "                    color=(255, 0, 0), thickness=5)\n",
        "    \n",
        "    return frame\n",
        "\n",
        "stream = cv2.VideoCapture(0)\n",
        "\n",
        "if not stream.isOpened():\n",
        "    print(\"No stream :(\")\n",
        "    exit()\n",
        "\n",
        "fps = stream.get(cv2.CAP_PROP_FPS)\n",
        "width = int(stream.get(3))\n",
        "height = int(stream.get(4))\n",
        "\n",
        "# list of FourCC video codes: https://softron.zendesk.com/hc/en-us/articles/207695697-List-of-FourCC-codes-for-video-codecs\n",
        "output = cv2.VideoWriter(\"assets/6_facial_detection.mp4\",\n",
        "            cv2.VideoWriter_fourcc('m', 'p', '4', 'v'),\n",
        "            fps=fps, frameSize=(width, height))\n",
        "\n",
        "while(True):\n",
        "    ret, frame = stream.read()\n",
        "    if not ret:\n",
        "        print(\"No more stream :(\")\n",
        "        break\n",
        "    \n",
        "    frame = detect_features(frame)\n",
        "    output.write(frame)\n",
        "    cv2.imshow(\"Webcam!\", frame)\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "stream.release()\n",
        "cv2.destroyAllWindows() #!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from mtcnn import MTCNN\n",
        "\n",
        "# Initialize MTCNN for face detection\n",
        "detector = MTCNN()\n",
        "\n",
        "def detect_features(frame):\n",
        "    # Convert frame to RGB (MTCNN expects RGB images)\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Detect faces using MTCNN\n",
        "    faces = detector.detect_faces(rgb_frame)\n",
        "    \n",
        "    # Draw rectangles around detected faces\n",
        "    for face in faces:\n",
        "        x, y, w, h = face['box']\n",
        "        frame = cv2.rectangle(frame, (x, y), (x+w, y+h), color=(0, 255, 0), thickness=5)\n",
        "        \n",
        "        # Draw rectangles around detected eyes (optional)\n",
        "        for key, value in face['keypoints'].items():\n",
        "            frame = cv2.rectangle(frame, (value[0]-5, value[1]-5), (value[0]+5, value[1]+5), color=(255, 0, 0), thickness=2)\n",
        "    \n",
        "    return frame\n",
        "\n",
        "stream = cv2.VideoCapture(0)\n",
        "\n",
        "if not stream.isOpened():\n",
        "    print(\"No stream :(\")\n",
        "    exit()\n",
        "\n",
        "fps = stream.get(cv2.CAP_PROP_FPS)\n",
        "width = int(stream.get(3))\n",
        "height = int(stream.get(4))\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "output = cv2.VideoWriter(\"assets/6_facial_detection.mp4\",\n",
        "            cv2.VideoWriter_fourcc('m', 'p', '4', 'v'),\n",
        "            fps=fps, frameSize=(width, height))\n",
        "\n",
        "while(True):\n",
        "    ret, frame = stream.read()\n",
        "    if not ret:\n",
        "        print(\"No more stream :(\")\n",
        "        break\n",
        "    \n",
        "    frame = detect_features(frame)\n",
        "    output.write(frame)\n",
        "    cv2.imshow(\"Webcam!\", frame)\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "stream.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from mtcnn import MTCNN\n",
        "from facenet_pytorch import MTCNN as FaceNetMTCNN, InceptionResnetV1\n",
        "from PIL import Image as PILImage  # Alias PIL's Image\n",
        "from torchvision import transforms\n",
        "from IPython.display import display, Image as IPyImage  # Alias IPython's Image\n",
        "import io\n",
        "\n",
        "\n",
        "\n",
        "# Initialize MTCNN from facenet_pytorch\n",
        "mtcnn = FaceNetMTCNN()\n",
        "facenet_model = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "# Load the sample picture\n",
        "sample_image_path = r\"C:\\Users\\kamka\\OneDrive\\Desktop\\CMP6200 Project\\Facial Recognition using MTCNN and FaceNet\\cropped_imgdata\\Karhou2.jpg\"\n",
        "sample_image = Image.open(sample_image_path)\n",
        "\n",
        "# Convert the PIL Image to a tensor and normalize it\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),  # Resize to the input size that FaceNet expects\n",
        "    transforms.ToTensor(),  # This also scales pixel values to [0, 1]\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust these values based on the expected normalization parameters for your model\n",
        "])\n",
        "\n",
        "sample_image_tensor = transform(sample_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Extract face embeddings from the sample picture\n",
        "sample_image_embedding = facenet_model(sample_image_tensor).detach().numpy()\n",
        "\n",
        "# Define a threshold for face similarity\n",
        "threshold = 0.6\n",
        "\n",
        "def detect_and_match_faces(frame):\n",
        "    # Detect faces using MTCNN\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "    \n",
        "    if boxes is not None:\n",
        "        for box in boxes:\n",
        "            # Extract face from the frame\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            face = frame[y1:y2, x1:x2]\n",
        "            \n",
        "            # Convert the face to a PIL Image and apply transforms\n",
        "            face_image_pil = Image.fromarray(face)\n",
        "            face_image_tensor = transform(face_image_pil).unsqueeze(0)  # Apply the same transformations\n",
        "            \n",
        "            # Extract face embedding from the detected face\n",
        "            face_embedding = facenet_model(face_image_tensor).detach().numpy()\n",
        "\n",
        "            # Calculate similarity between the embeddings\n",
        "            similarity = cosine_similarity(sample_image_embedding, face_embedding)[0][0]\n",
        "            \n",
        "            # Display indications based on similarity\n",
        "            if similarity >= threshold:\n",
        "                cv2.putText(frame, \"Match\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "            else:\n",
        "                cv2.putText(frame, \"No match\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "    \n",
        "    return frame\n",
        "\n",
        "def show_image_in_notebook(image):\n",
        "    is_success, buffer = cv2.imencode(\".jpg\", image)\n",
        "    io_buf = io.BytesIO(buffer)\n",
        "    display(Image(io_buf.read()))\n",
        "\n",
        "stream = cv2.VideoCapture(0)\n",
        "\n",
        "if not stream.isOpened():\n",
        "    print(\"No stream :(\")\n",
        "    exit()\n",
        "\n",
        "fps = stream.get(cv2.CAP_PROP_FPS)\n",
        "width = int(stream.get(3))\n",
        "height = int(stream.get(4))\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "output = cv2.VideoWriter(\"assets/6_facial_detection.mp4\",\n",
        "            cv2.VideoWriter_fourcc('m', 'p', '4', 'v'),\n",
        "            fps=fps, frameSize=(width, height))\n",
        "\n",
        "while True:\n",
        "    ret, frame = stream.read()\n",
        "    if not ret:\n",
        "        print(\"No more stream :(\")\n",
        "        break\n",
        "    \n",
        "    frame = detect_and_match_faces(frame)\n",
        "    output.write(frame)\n",
        "    #cv2.imshow(\"Webcam\", frame)\n",
        "    show_image_in_notebook(frame)\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "stream.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kamka\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing frame: tile cannot extend outside image\n",
            "Error processing frame: tile cannot extend outside image\n",
            "Error processing frame: tile cannot extend outside image\n",
            "Error processing frame: tile cannot extend outside image\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
        "from PIL import Image as PILImage\n",
        "from torchvision import transforms\n",
        "from IPython.display import display, Image as IPyImage\n",
        "import io\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initialize MTCNN for face detection \n",
        "mtcnn = MTCNN()\n",
        "# Initialize FaceNet model\n",
        "facenet_model = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "# Load the sample image and preprocess it\n",
        "sample_image_path = r\"C:\\Users\\kamka\\OneDrive\\Desktop\\CMP6200 Project\\Facial Recognition using MTCNN and FaceNet\\cropped_imgdata\\Karhou2.jpg\"\n",
        "sample_image = PILImage.open(sample_image_path)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "sample_image_tensor = transform(sample_image).unsqueeze(0)\n",
        "sample_image_embedding = facenet_model(sample_image_tensor).detach().numpy()\n",
        "\n",
        "# Define a threshold for face similarity\n",
        "threshold = 0.6\n",
        "\n",
        "# Define function for detect face and match face similarity\n",
        "def detect_and_match_faces(frame):\n",
        "    try:\n",
        "        # Detect faces using MTCNN\n",
        "        boxes, _ = mtcnn.detect(frame)\n",
        "\n",
        "        if boxes is not None:\n",
        "            for box in boxes:\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "                face = frame[y1:y2, x1:x2]\n",
        "\n",
        "                # Convert face to PIL Image and preprocess\n",
        "                face_image_pil = PILImage.fromarray(face)\n",
        "                face_image_tensor = transform(face_image_pil).unsqueeze(0)\n",
        "\n",
        "                # Extract face embedding using FaceNet Model\n",
        "                face_embedding = facenet_model(face_image_tensor).detach().numpy()\n",
        "\n",
        "                # Calculate similarity\n",
        "                similarity = cosine_similarity(sample_image_embedding, face_embedding)[0][0]\n",
        "\n",
        "                # Display match indication based on similarity\n",
        "                if similarity >= threshold:\n",
        "                    cv2.putText(frame, \"Match\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "                else:\n",
        "                    cv2.putText(frame, \"No match\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing frame: {e}\")\n",
        "\n",
        "    return frame\n",
        "\n",
        "def show_image_in_notebook(image):\n",
        "    is_success, buffer = cv2.imencode(\".jpg\", image)\n",
        "    io_buf = io.BytesIO(buffer)\n",
        "    display(IPyImage(data=io_buf.read(), format='jpg')) \n",
        "\n",
        "# Open video capture\n",
        "stream = cv2.VideoCapture(0)\n",
        "\n",
        "if not stream.isOpened():\n",
        "    print(\"No stream :(\")\n",
        "    exit()\n",
        "\n",
        "fps = stream.get(cv2.CAP_PROP_FPS)\n",
        "width = int(stream.get(3))\n",
        "height = int(stream.get(4))\n",
        "\n",
        "# Define VideoWriter object\n",
        "output = cv2.VideoWriter(\"assets/6_facial_detection.mp4\", cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), fps=fps, frameSize=(width, height))\n",
        "\n",
        "while True:\n",
        "    ret, frame = stream.read()\n",
        "    if not ret:\n",
        "        print(\"No more stream :(\")\n",
        "        break\n",
        "\n",
        "    frame = detect_and_match_faces(frame)\n",
        "    output.write(frame)\n",
        "    cv2.imshow(\"Webcam\", frame)\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "stream.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
